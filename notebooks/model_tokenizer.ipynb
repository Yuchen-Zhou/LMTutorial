{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 模型与分词器\n",
    "\n",
    "## 模型\n",
    "除了`AutoModel`根据checkpoint自动加载模型以外，也可以直接使用模型对应的`Model`类，例如BERT对应的就是`BertModel`:"
   ],
   "id": "77ad13616eeee20e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T07:26:31.471196Z",
     "start_time": "2025-02-19T07:24:23.790644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "f43538cc537256d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bfcfaee8aa24297b74aa7b42ea9b10e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "注意，在大部分情况下，我们都应该使用`AutoModel`来加载模型。\n",
    "\n",
    "### 加载模型\n",
    "所有存储在HuggingFace都可以使用`AutoModel.from_pretrained()`来加载权重，参数可以像上"
   ],
   "id": "134ffbd67d5c4fa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:25:23.099092Z",
     "start_time": "2025-02-19T12:25:22.936988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('../models/bert')"
   ],
   "id": "9cb8dd3f4bd3f27a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "部分模型的 Hub 页面中会包含很多文件，我们通常只需要下载模型对应的 config.json 和 pytorch_model.bin，以及分词器对应的 tokenizer.json、tokenizer_config.json 和 vocab.txt。\n",
    "\n",
    "\n",
    "### 保存模型\n",
    "保存模型通过调用`Model.save_pretrained()`函数实现，例如保存加载的BERT模型："
   ],
   "id": "5e3367f44ffe022b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:05:56.268018Z",
     "start_time": "2025-02-19T13:05:51.050846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('bert-base-cased')\n",
    "model.save_pretrained('../models/bert-base-cased')"
   ],
   "id": "f00d51b9b73d3b7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这会在保存路径下创建两个文件：\n",
    "- config.json: 模型配置文件，存储模型结果参数，如Transformer层数、特征空间维度\n",
    "- pytorch_model.bin: 又称state dictionary，存储模型的权重\n",
    "\n",
    "简单来说，配置文件记录模型的结构，模型权重记录模型的参数，这两个文件缺一不可。我们自己保存的模型同样通过`Model.from_pretrained()`加载，只需要传递保存目录的路径。\n",
    "\n"
   ],
   "id": "b3eb493f253e6ba6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 分词器\n",
    "由于神经网络模型不能直接处理文本，因此我们需要先将文本转换为数字，这个过程被称为编码(Encoding)，其中包含两个步骤：\n",
    "1. 使用分词器(tokenizer)将文本按词、子词、字符切分为tokens\n",
    "2. 将所有的token映射到对应的token ID\n",
    "\n",
    "### 分词策略\n",
    "根据切分粒度的不同，分词策略可以分为以下几种：\n",
    "- 按词切分：可以直接使用Python的`split()`函数按空格进行分词：\n"
   ],
   "id": "3d39fa67b0f05743"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:53:40.494961Z",
     "start_time": "2025-02-19T12:53:40.492097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenizer_text)"
   ],
   "id": "426b39a67ecc6457",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这种方式会导致生成巨大的词表。\n",
    "\n",
    "> 词表就是一个映射字典，负责将token映射到对应的ID(从0开始)。神经网络模型就是通过这些token ID来区分每一个token。\n",
    "\n",
    "当遇到不在词表中的词时，分词器会使用一个专门的`[UNK] token`来表示它是unknown的。\n"
   ],
   "id": "39e47f978370f67b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 按字符切分(Character-based)：将文本切分为每个字符\n",
    "- 按子词切分(SubWord)：高频词直接保留，低频词被切分为更有意义的子词。例如\"annoyingly\"是一个低频词，可以切分\"annoying\"和\"ly\"，这两个子词不仅出现频率更高，而且词义也得以保留。"
   ],
   "id": "e28493cef22fa60b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 加载与保存分词器\n",
    "分词器的加载与保存与模型相似，使用`Tokenizer.from_pretrained()`和`Tokenizer.save_pretrained()`函数。例如加载并保存BERT模型的分词器："
   ],
   "id": "8c79ed2184076bce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:06:28.065367Z",
     "start_time": "2025-02-19T13:06:07.980149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.save_pretrained('../models/bert-base-cased')"
   ],
   "id": "d4d2ff4b04b845e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert-base-cased/tokenizer_config.json',\n",
       " '../models/bert-base-cased/special_tokens_map.json',\n",
       " '../models/bert-base-cased/vocab.txt',\n",
       " '../models/bert-base-cased/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "同样地，在大部分情况下我们都应该使用`AutoTokenizer`来加载分词器：",
   "id": "94443e947690fe7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:08:51.929068Z",
     "start_time": "2025-02-19T13:08:31.845231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.save_pretrained('../models/bert-base-cased')"
   ],
   "id": "91926f9a85a408df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert-base-cased/tokenizer_config.json',\n",
       " '../models/bert-base-cased/special_tokens_map.json',\n",
       " '../models/bert-base-cased/vocab.txt',\n",
       " '../models/bert-base-cased/added_tokens.json',\n",
       " '../models/bert-base-cased/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "调用`Tokenizer.save_pretrained()`函数会保存在路径下创建三个文件：\n",
    "- special_tokens_map.json: 映射文件，里面包含unknown token等特殊字符的映射关系\n",
    "- tokenizer_config.json: 分词器配置文件，存储构建分词器需要的参数\n",
    "- vocab.txt: 词表，一行一个token，行号就是对应的token ID(从0开始)\n",
    "\n",
    "\n",
    "### 编码与解码文件\n",
    "文本编码(Encoding)过程包含两个步骤：\n",
    "- 分词: 使用分词器某种策略将文本切分为tokens\n",
    "- 映射: 将tokens转化为对应的token IDs\n",
    "\n",
    "下面我们首先使用BERT分词器来对文本进行分词："
   ],
   "id": "81e22bc099a43e84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:25:08.114983Z",
     "start_time": "2025-02-19T13:24:48.050809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "sequence = \"Using a transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ],
   "id": "f313dba96d49abe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可以看到，BERT分词器采用的是子词切分策略，它会不断切分词语直到获得词表中的token，例如“transformer”会被切分为“transform”和“##er”\n",
    "\n",
    "然后，我们通过`convert_tokens_to_ids()`将切分出的tokens转换为对应的token IDs："
   ],
   "id": "b40f195b627dbf67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:26:42.230837Z",
     "start_time": "2025-02-19T13:26:42.228173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ],
   "id": "3156201045098af0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 11303, 1200, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "还可以通过`encode()`函数将这两个步骤合并，并且`encode()`会自动添加模型需要的特殊token，例如BERT分词会分别在序列的首尾添加[CLS]和[SEP]",
   "id": "2d9af03414975d2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:33:42.490620Z",
     "start_time": "2025-02-19T13:33:22.423149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "sequence = \"Using a transformer network is simple\"\n",
    "sequence_ids = tokenizer.encode(sequence)\n",
    "\n",
    "print(sequence_ids)"
   ],
   "id": "c88421c7f45f306f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "还可以通过`encode()`函数将这两个步骤合并，并且`encode()`会自动添加模型的特殊token，例如BERT分词器会分别在序列的首尾添加[CLS]和[SEP]:",
   "id": "c88c8740a755171c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:38:50.980202Z",
     "start_time": "2025-02-19T13:38:30.909309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "sequence = 'Using a transformer network is simple'\n",
    "sequence_ids = tokenizer.encode(sequence)\n",
    "\n",
    "print(sequence_ids)"
   ],
   "id": "b31bc05e10ed8561",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:40:23.841116Z",
     "start_time": "2025-02-19T13:40:03.767982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在实际编码文本时，最常见的是直接使用分词器进行处理\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_text = tokenizer('Using a transformer network is simple')\n",
    "print(tokenizer_text)"
   ],
   "id": "b2c4daf65c8b4f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "文本解码(Decoding)与编码相反，负责将token IDs转换回原来的字符串。注意，解码过程不是简单地将token IDs映射会tokens，还需要合并那些被分为多个token的单词。下面我们通过`decode()`函数解码前面生成的token IDs：",
   "id": "8159d381761f1ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:46:58.080610Z",
     "start_time": "2025-02-19T13:46:38.026635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "decode_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decode_string)\n",
    "\n",
    "decoded_string = tokenizer.decode([101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102])\n",
    "print(decoded_string)"
   ],
   "id": "dcef20d64473eafc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n",
      "[CLS] Using a Transformer network is simple [SEP]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 处理多段文本\n",
    "\n",
    "现实场景中，我们往往会同时处理多段文本，而且模型也只接受批(batch)数据作为输入，即使只有一段文本，也需要将它组成一个只包含一个样本的batch"
   ],
   "id": "b50c4e0a4875bc79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:53:07.922166Z",
     "start_time": "2025-02-19T13:52:57.542038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for HuggingFace course my whole life\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs: \\n\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits: \\n\", output.logits)"
   ],
   "id": "f6bb98665c2b4356",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: \n",
      " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005, 17662, 12172,  2607,  2026,\n",
      "          2878,  2166]])\n",
      "Logits: \n",
      " tensor([[-1.9676,  2.1872]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:56:38.368037Z",
     "start_time": "2025-02-19T13:56:28.252317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 实际场景中，我们直接使用文词器对文本进行处理\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for HuggingFace course my whole life\"\n",
    "\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors='pt')\n",
    "print(\"Inputs Keys: \\n\", tokenized_inputs.keys())\n",
    "print(\"\\nInput IDs: \\n\", tokenized_inputs['input_ids'])\n",
    "\n",
    "output = model(**tokenized_inputs)\n",
    "print(\"\\nLogits: \\n\", output.logits)"
   ],
   "id": "ca1ca5b81e6b1f59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs Keys: \n",
      " dict_keys(['input_ids', 'attention_mask'])\n",
      "\n",
      "Input IDs: \n",
      " tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,   102]])\n",
      "\n",
      "Logits: \n",
      " tensor([[-2.1236,  2.1398]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Padding操作\n",
    "\n",
    "按批输入多段文本产生的一个直接问题就是：batch中的文本有长有短，而输入张量必须是严格的二维矩形，维度为(batch size, sequence length)，即每一段文本编码后的token IDs数量必须一样多。"
   ],
   "id": "b643b9d24aef413f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:02:52.997267Z",
     "start_time": "2025-02-20T01:02:52.993365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 下面这样的ID列表是无法转换为张量的\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ],
   "id": "5758bfbd16b0a6c7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:07:07.814632Z",
     "start_time": "2025-02-20T01:07:07.812021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 需要通过padding操作\n",
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ],
   "id": "ed440659e965ba2e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:10:43.273141Z",
     "start_time": "2025-02-20T01:10:22.918447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型的padding ID可以通过其分词器的`pad_token_id`属性获得\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "id": "74d9bf97740c0027",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Attention Mask\n",
    "Attention Mask是一个尺寸与input IDs完全相同，且仅由0和1组成的张量，0表示对应位置的token是填充符，不参与计算。当然，一些特殊的模型结构也会借助Attention Mask来遮蔽掉指定的tokens"
   ],
   "id": "9a3b500b486b34cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:18:01.348599Z",
     "start_time": "2025-02-20T01:17:41.013502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "batched_attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "outputs = model(\n",
    "    torch.tensor(batched_ids),\n",
    "    attention_mask=torch.tensor(batched_attention_mask),\n",
    ")\n",
    "print(outputs.logits)"
   ],
   "id": "ca479e829567bf99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "目前大部分Transformer模型只能接受长度不超过512或1024的token序列，因此长序列有三种处理方法：\n",
    "1. 使用一个支持长文的Transformer模型，例如Longformer和LED(最大4096)\n",
    "2. 设定最大长度`max_sequence_length`以截断输入序列：`sequence = sequence[:max_sequence_length]`\n",
    "3. 将长文切片为短文本块(chunk)，然后分别对每一个chunk编码\n",
    "\n",
    "### 直接使用分词器"
   ],
   "id": "8af880e895fa884a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:24:07.043594Z",
     "start_time": "2025-02-20T01:23:56.960704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = [\n",
    "    \"I've been waiting for HuggingFace course my whole life\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs)"
   ],
   "id": "a29f4293c5da42b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 17662, 12172, 2607, 2026, 2878, 2166, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Padding操作通过`padding`参数来控制：\n",
    "- `padding=\"longest\"`：将序列填充到当前batch中最长序列的长度\n",
    "- `padding=\"max_length\"`：将所有序列填充到模型能够接受的最大长度"
   ],
   "id": "5c7d4167a458724f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:28:03.480789Z",
     "start_time": "2025-02-20T01:27:53.419593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = [\n",
    "    \"I've been waiting for HuggingFace course my whole life\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequence, padding=\"longest\")\n",
    "print(model_inputs)\n",
    "\n",
    "model_inputs = tokenizer(sequence, padding=\"max_length\")\n",
    "print(model_inputs)"
   ],
   "id": "c7f42bff79154ce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 17662, 12172, 2607, 2026, 2878, 2166, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 17662, 12172, 2607, 2026, 2878, 2166, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**截断操作**通过`truncation`参数来控制，如果`truncation=True`，那么大于模型最大接受长度的序列都会被截断",
   "id": "32f804ff60124f74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:31:57.706924Z",
     "start_time": "2025-02-20T01:31:47.644945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = [\n",
    "    \"I've been waiting for HuggingFace course my whole life\",\n",
    "    \"So have I!\"\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer(sequence, max_length=8, truncation=True)\n",
    "print(model_inputs)"
   ],
   "id": "af633e38d6e84c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:33:39.794363Z",
     "start_time": "2025-02-20T01:33:39.783807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 分词器还可以指定返回的张量格式：pt为PyTorch张量，tf为Tensorflow张量，np为NumPy\n",
    "model_inputs = tokenizer(sequence, padding=True, return_tensors='pt')\n",
    "print(model_inputs)\n",
    "\n",
    "model_inputs = tokenizer(sequence, padding=True, return_tensors='np')\n",
    "print(model_inputs)"
   ],
   "id": "bb3f310af5b111b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005, 17662, 12172,\n",
      "         2607,  2026,  2878,  2166,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:34:37.653632Z",
     "start_time": "2025-02-20T01:34:37.628831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 实际使用分词器时，我们通常会同时进行 padding 操作和截断操作，并设置返回格式为 Pytorch 张量，这样就可以直接将分词结果送入模型：\n",
    "\n",
    "tokens = tokenizer(sequence, padding=True, truncation=True, return_tensors='pt')\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output.logits)"
   ],
   "id": "98fdcc58a6ee763d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[-2.1236,  2.1398],\n",
      "        [-3.6183,  3.9138]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 编码句子对\n",
    "除了对单段文本进行编码以外（batch 只是并行地编码多个单段文本），对于 BERT 等包含“句子对”预训练任务的模型，它们的分词器都支持对“句子对”进行编码"
   ],
   "id": "1fdd9dcb9829658d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:41:22.758543Z",
     "start_time": "2025-02-20T01:41:22.744135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "inputs = tokenizer('This is the first sentence', 'This is the second sentence')\n",
    "print(inputs)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'])\n",
    "print(tokens)"
   ],
   "id": "ea5d46211e5d152e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1188, 1110, 1103, 1148, 5650, 102, 1188, 1110, 1103, 1248, 5650, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'This', 'is', 'the', 'first', 'sentence', '[SEP]', 'This', 'is', 'the', 'second', 'sentence', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 如果选择其他模型，分词器的输出不一定会包含`token_type_ids`项。分词器只需保证输出格式与模型训练时的输入一致即可",
   "id": "238ee9ec0f039cd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:53:16.960570Z",
     "start_time": "2025-02-20T01:53:16.939626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 实际使用，不需要去关注编码结果中是否包含 token_type_ids 项，分词器会根据checkpoint自动调整输出格式\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence1_list = [\"First sentence.\", \"This is the second sentence.\", \"Third one.\"]\n",
    "sentence2_list = [\"First sentence is short.\", \"The second sentence is very very very long.\", \"ok.\"]\n",
    "\n",
    "tokens = tokenizer(\n",
    "    sentence1_list,\n",
    "    sentence2_list,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(tokens)\n",
    "print(tokens['input_ids'].shape)"
   ],
   "id": "ff00e59cc6485317",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1752,  5650,   119,   102,  1752,  5650,  1110,  1603,   119,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1188,  1110,  1103,  1248,  5650,   119,   102,  1109,  1248,\n",
      "          5650,  1110,  1304,  1304,  1304,  1263,   119,   102],\n",
      "        [  101,  4180,  1141,   119,   102, 21534,   119,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([3, 18])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 添加Token\n",
    "实际操作中，我们还经常会遇到输入中需要包含特殊标记符的情况，例如使用 [ENT_START] 和 [ENT_END] 标记出文本中的实体。由于这些自定义 token 并不在预训练模型原来的词表中，因此直接运用分词器处理就会出现问题。"
   ],
   "id": "b2db2000e804ecea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T01:56:21.752148Z",
     "start_time": "2025-02-20T01:56:21.750170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = 'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'\n",
    "print(tokenizer.tokenize(sentence))"
   ],
   "id": "fe390edb2337c323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', '[', 'E', '##NT', '_', 'ST', '##AR', '##T', ']', 'cars', '[', 'E', '##NT', '_', 'E', '##ND', ']', 'collided', 'in', 'a', '[', 'E', '##NT', '_', 'ST', '##AR', '##T', ']', 'tunnel', '[', 'E', '##NT', '_', 'E', '##ND', ']', 'this', 'morning', '.']\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 添加新token\n",
    "Transformer库提供了两种方式来添加新token，分别是：\n",
    "- `add_tokens()`添加普通token：参数是新token列表，如果token不在词表中，就会被添加到词表的最后"
   ],
   "id": "823d18f7cafaa7e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:02:28.209963Z",
     "start_time": "2025-02-20T02:02:28.195006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(['new_token1', 'new_token2'])\n",
    "print('I have added', num_added_toks, 'tokens')"
   ],
   "id": "d885d51b40bd53c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have added 2 tokens\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:03:39.608688Z",
     "start_time": "2025-02-20T02:03:39.597696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 为了防止token已经包含在词表中，还可以预先对新token列表进行过滤\n",
    "new_tokens = ['new_token1', 'new_token2']\n",
    "new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "tokenizer.add_tokens(list(new_tokens))"
   ],
   "id": "66111bdaade19da7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- `add_special_tokens()`添加特殊token：参数是包含特殊token的字典，键值只能从`bos_token`,`eos_token`,`unk_token`,`sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`中选择。如果token不在词表中，就会被添加到词表的最后。添加后，还可以通过特殊属性来访问这些token，例如`tokenizer.cls_token`就指向cls token",
   "id": "96e039017b660c2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:08:53.463513Z",
     "start_time": "2025-02-20T02:08:53.443585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sepcial_tokens_dict = {'cls_token': '[MY_CLS]'}\n",
    "\n",
    "num_added_toks = tokenizer.add_special_tokens(sepcial_tokens_dict)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    "\n",
    "assert tokenizer.cls_token == '[MY_CLS]'"
   ],
   "id": "7f02a5516355010f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 1 tokens\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:13:18.109553Z",
     "start_time": "2025-02-20T02:13:18.093794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 我们也可以使用add_tokens() 添加特殊token，只需要额外设置参数special_tokens=True\n",
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(['[NEW_tok1]', '[NEW_tok2]'])\n",
    "num_added_toks = tokenizer.add_tokens([\"[NEW_tok3]\", \"[NEW_tok4]\"], special_tokens=True)\n",
    "\n",
    "print(\"We have added\", num_added_toks, 'tokens')\n",
    "print(tokenizer.tokenize('[NEW_tok1] Hello [NEW_tok2] [NEW_tok3] World [NEW_tok4]!'))"
   ],
   "id": "6451c422bd41507c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 2 tokens\n",
      "['[NEW_tok1]', 'Hello', '[NEW_tok2]', '[NEW_tok3]', 'World', '[NEW_tok4]', '!']\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 特殊token的标准化(normalization)与普通token有一些不同，比如不会被小写",
   "id": "6fcf83b1f67d61e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:28:10.035467Z",
     "start_time": "2025-02-20T02:28:10.010581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(['[ENT_START]', '[ENT_END]'], special_tokens=True)\n",
    "\n",
    "print('I have added', num_added_toks, 'tokens')\n",
    "\n",
    "sentence = 'Two [ENT_START] cars [ENT_END] collided in a [ENT_START] tunnel [ENT_END] this morning.'\n",
    "print(tokenizer.tokenize(sentence))"
   ],
   "id": "1662610bd683fe34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have added 2 tokens\n",
      "['Two', '[ENT_START]', 'cars', '[ENT_END]', 'collided', 'in', 'a', '[ENT_START]', 'tunnel', '[ENT_END]', 'this', 'morning', '.']\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 调整embedding矩阵\n",
    ">> 向词表中添加新的token后，必须重置模型embedding矩阵的大小，也就是向矩阵中添加新token对应的embedding，这样模型才可以正常工作，将token映射到对应的embedding\n",
    "\n",
    "调整embedding矩阵通过`resize_token_embeddings()`函数来实现"
   ],
   "id": "978e77a219a27779"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:34:28.237178Z",
     "start_time": "2025-02-20T02:34:27.861818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"../models/bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "num_added_toks = tokenizer.add_tokens(['[ENT_START]', '[ENT_END]'], special_tokens=True)\n",
    "print(\"After we add\", num_added_toks, \"tokens\")\n",
    "print('vocabulary size:', len(tokenizer))\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.embeddings.word_embeddings.weight.size())\n",
    "\n",
    "# Randomly generated matrix\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "75aae8e29f5aa27a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 28996\n",
      "After we add 2 tokens\n",
      "vocabulary size: 28998\n",
      "torch.Size([28998, 768])\n",
      "tensor([[-0.0081, -0.0199, -0.0088,  ..., -0.0140, -0.0205, -0.0110],\n",
      "        [-0.0081, -0.0199, -0.0088,  ..., -0.0140, -0.0205, -0.0110]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Token embedding 初始化\n",
    "如果有充分的语料对模型进行微调或者继续预训练，那么将新添加 token 初始化为随机向量没什么问题。但是如果训练语料较少，甚至是只有很少语料的 few-shot learning 场景下，这种做法就存在问题。研究表明，在训练数据不够多的情况下，这些新添加 token 的 embedding 只会在初始值附近小幅波动。换句话说，即使经过训练，它们的值事实上还是随机的。\n",
    "### 直接赋值\n"
   ],
   "id": "b35fbf6d3c0e45f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:41:29.308282Z",
     "start_time": "2025-02-20T02:41:29.300805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.embeddings.word_embeddings.weight[-2:, :] = torch.zeros([2, model.config.hidden_size], requires_grad=True)\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "61d5599f6c308fcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "注意，初始化 embedding 的过程并不可导，因此这里通过 torch.no_grad() 暂停梯度的计算。",
   "id": "a0eb8e3cba38415e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:43:59.170569Z",
     "start_time": "2025-02-20T02:43:59.166322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 现实场景中，更为常见的做法是使用已有 token 的 embedding 来初始化新添加 token。\n",
    "import torch\n",
    "\n",
    "token_id = tokenizer.convert_tokens_to_ids('entity')\n",
    "token_embedding = model.embeddings.word_embeddings.weight[token_id]\n",
    "print(token_id)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(1, num_added_toks + 1):\n",
    "        model.embeddings.word_embeddings.weight[-i, :] = token_embedding.clone().detach().requires_grad_(True)\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "3ce0476c18aabe21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9127\n",
      "tensor([[-0.0277, -0.0564,  0.0192,  ..., -0.0485, -0.0089, -0.0457],\n",
      "        [-0.0277, -0.0564,  0.0192,  ..., -0.0485, -0.0089, -0.0457]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 初始化为已有token的值\n",
    "更高级的做法是根据新添加的token的语义来进行初始化。例如将值初始化为token语义描述中所有token的平均值，假设新token $t_i$的语义描述为$w_{i,1},w_{i,2},...,w_{i,n}$，那么初始化$t_i$，那么初始化$t_i$的embedding为$E(t_i)=\\frac{1}{n} \\sum_{j=1}^{n},E(w_{i, j})$\n",
    "\n",
    "这里$E$表示预训练模型的embedding矩阵"
   ],
   "id": "313bbf39f37d3c73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T02:52:44.147859Z",
     "start_time": "2025-02-20T02:52:44.138246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "descriptions = ['start of entity', 'end of entity']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, token in enumerate(reversed(descriptions), start=1):\n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        print(tokenized)\n",
    "        tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        new_embedding = model.embeddings.word_embeddings.weight[tokenized_ids].mean(axis=0)\n",
    "        model.embeddings.word_embeddings.weight[-i, :] = new_embedding.clone().detach().requires_grad_(True)\n",
    "print(model.embeddings.word_embeddings.weight[-2:, :])"
   ],
   "id": "85612c8530633ca3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['end', 'of', 'entity']\n",
      "['start', 'of', 'entity']\n",
      "tensor([[-0.0060, -0.0084, -0.0003,  ..., -0.0499, -0.0044, -0.0052],\n",
      "        [-0.0324, -0.0079, -0.0157,  ..., -0.0434, -0.0019,  0.0049]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7a06b1a7b35685ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
